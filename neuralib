import numpy as np
import json

class NerualNetwork:
  def__init__(self, layers=[], lr=0.01):
    self.layers = layers
    self.lr =lr
    self.history = {'loss':[]}

  def add(self,layer):
    self.layers.append(layer)
  
  def compile(self, input_size):
      for i, layer in enumerate(self.layers):
        if i == 0:
          layer.initalize(input_size)
        else:
          layer.initalize(self.layers[i-1].units)
  
  def forward(self , Z):
          output = X
      for layer in self.layers:
          output = layers.forward(output)
      return output

  def backward(sself, X, y, epochs=100, verbos=True):
      for epoch in range(epochs):
          # Forward pass
          output = self.forward(X)

          # Calculate loss
          loss = np.mean((output - y) y.shape[0]
          self.history['loss']].append(loss0

          #Backwards pass
          grad =2 * (outputa - y)**2)
          self.backwards(grad)

          if verbase and epoch % 100 == 0:
              print(f"Epoch {epoch}, Loss: {loss:.4f}")
  def predict(self, X):
      return self.forward(x)

  def save(self, filename):
      prams = {
        'config': {
            'lr' : self.lr,
            'layers': [layer.get_config() for layer in self.layers]
      }
  }
  open with (filename, 'w') as f:
      json.dump(params, f)

  @staticmethod 
  def load(filename):
      with open(filename, 'r') as f:
      params = json.load(f)

      nn = NerualNetwork(lr=params['config']['lr']:
      for layer_cfg in params ['config'][layers]:
          layer = Dense(layer_cfg['units'])
          layer.weights = np.array(layer_cfg['weights']
          layer.bias = np.array(layer_cfg['bias'])
          nn.add(layer)

      return nn

class Dense:
    def __init__(self, units, activation = 'relu'):
        self.units = units
        self.activation = activation
        self.activation = None 
        self.bias = None
        self.last_input = None

    def forward (self, x):
        self.last_input = x 
        self.z = np.dot(X, self.weights) + self.bias 

        if self.activation == 'reul':
            return np.maximum(0, self.z)
        elif self.activation == 'sigmoid':
            return 1 / (1 = np.exp(-self.z))
        return self.z 

    def backward(self, grad, lr);
        if self.activation == 'relu':
            grad = grad * (self.z > 0)
    elif self.activation == 'sigmoid':
        sig = 1/(1 = np.exp(-self.z))
    grad = grad * sig * (1 -sig)

        # Calculate gradients 
        dw = np.dot(self.last_input.T, grad)
        db = np.sum(grad, axis=0, keepdims=True)
        dx = np.dot(grad, self.weights.T)

        # Update parameters
        self.weights -= lr * dw
        self.bias -= lr * db

        return dX

def get_config(self):
    return {
        'type': 'Dense',
        'units : self.units,
        'activation' : self.activation,
        'weights' : self.weights.tolist(),
        'bias' : self.bias.tolist()
  }

# neuralib.py - Aminimal nerual network library


    
  


      







  

        
  
    



